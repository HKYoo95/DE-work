{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 activity 로그를 Hive table 로 제공하기 위한 Spark Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark 버전 및 연결 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/27 19:36:37 WARN Utils: Your hostname, Hwikeunui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 172.29.40.132 instead (on interface en0)\n",
      "\n",
      "\n",
      "24/05/27 19:36:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 16.0.1\n",
      "Branch HEAD\n",
      "Compiled by user liangchi on 2023-02-10T19:57:40Z\n",
      "Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.29.40.132:4041\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1716806200986)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!spark-submit --version\n",
    "//!spark-shell --version\n",
    "//!spark-sql --version\n",
    "\n",
    "//spark.version // 3.3.2\n",
    "//sc.version // 3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/28 15:29:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|2019-10-01 00:00:...|      view|  44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 00:00:...|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|  33.20|554748717|9333dfbd-b87a-470...|\n",
      "|2019-10-01 00:00:...|      view|  17200506|2053013559792632471|furniture.living_...|    null| 543.10|519107250|566511c2-e2e3-422...|\n",
      "|2019-10-01 00:00:...|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n",
      "|2019-10-01 00:00:...|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n",
      "|2019-10-01 00:00:...|      view|   1480613|2053013561092866779|   computers.desktop|  pulser| 908.62|512742880|0d0d91c2-c9c2-4e8...|\n",
      "|2019-10-01 00:00:...|      view|  17300353|2053013553853497655|                null|   creed| 380.96|555447699|4fe811e9-91de-46d...|\n",
      "|2019-10-01 00:00:...|      view|  31500053|2053013558031024687|                null|luminarc|  41.16|550978835|6280d577-25c8-414...|\n",
      "|2019-10-01 00:00:...|      view|  28719074|2053013565480109009|  apparel.shoes.keds|   baden| 102.71|520571932|ac1cd4e5-a3ce-422...|\n",
      "|2019-10-01 00:00:...|      view|   1004545|2053013555631882655|electronics.smart...|  huawei| 566.01|537918940|406c46ed-90a4-478...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@61507cd\n",
       "FILE_PATH: String = .//data//2019-Oct.csv\n",
       "df_test: org.apache.spark.sql.DataFrame = [event_time: string, event_type: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "                        .appName(\"DE Work\")\n",
    "                        .enableHiveSupport()\n",
    "                        .getOrCreate()\n",
    "\n",
    "val FILE_PATH = \".//data//2019-Oct.csv\"\n",
    "val df_test = spark.read.format(\"csv\")\n",
    "                        .option(\"header\",\"true\")\n",
    "                        .load(FILE_PATH)\n",
    "df_test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 처리\n",
    "\\* 사용데이터 : eCommerce behavior data from multi category store(kaggle)  \n",
    "  \\** 2019-Oct.csv (총 42,448,764건 / 5.67GB)  \n",
    "  \\** 2019-Nov.csv (총 67,501,979건 / 9.01GB)  \n",
    "  \\** 데이터 출처 : (https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store)\n",
    "\n",
    "- KST 기준 daily partition 처리\n",
    "- 재처리 후 parquet, snappy 처리\n",
    "- External Table 방식으로 설계\n",
    "- 추가 기간 처리에 대응가능하도록 구현\n",
    "- 배치 장애시 복구를 위한 장치 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터 로드**\n",
    "- inferschema 옵션을 false로 하고 스키마를 직접 선언하여 성능향상 가능(소요시간 단축)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/28 16:24:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|   brand|  price|  user_id|        user_session|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "|2019-10-01 09:00:00|      view|  44600062|2103807459595387724|                null|shiseido|  35.79|541312140|72d76fde-8bb3-4e0...|\n",
      "|2019-10-01 09:00:00|      view|   3900821|2053013552326770905|appliances.enviro...|    aqua|   33.2|554748717|9333dfbd-b87a-470...|\n",
      "|2019-10-01 09:00:01|      view|  17200506|2053013559792632471|furniture.living_...|    null|  543.1|519107250|566511c2-e2e3-422...|\n",
      "|2019-10-01 09:00:01|      view|   1307067|2053013558920217191|  computers.notebook|  lenovo| 251.74|550050854|7c90fc70-0e80-459...|\n",
      "|2019-10-01 09:00:04|      view|   1004237|2053013555631882655|electronics.smart...|   apple|1081.98|535871217|c6bd7419-2748-4c5...|\n",
      "|2019-10-01 09:00:05|      view|   1480613|2053013561092866779|   computers.desktop|  pulser| 908.62|512742880|0d0d91c2-c9c2-4e8...|\n",
      "|2019-10-01 09:00:08|      view|  17300353|2053013553853497655|                null|   creed| 380.96|555447699|4fe811e9-91de-46d...|\n",
      "|2019-10-01 09:00:08|      view|  31500053|2053013558031024687|                null|luminarc|  41.16|550978835|6280d577-25c8-414...|\n",
      "|2019-10-01 09:00:10|      view|  28719074|2053013565480109009|  apparel.shoes.keds|   baden| 102.71|520571932|ac1cd4e5-a3ce-422...|\n",
      "|2019-10-01 09:00:11|      view|   1004545|2053013555631882655|electronics.smart...|  huawei| 566.01|537918940|406c46ed-90a4-478...|\n",
      "+-------------------+----------+----------+-------------------+--------------------+--------+-------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@61507cd\n",
       "srcFilePath: String = ..//src_data//2019-Oct.csv\n",
       "df: org.apache.spark.sql.DataFrame = [event_time: timestamp, event_type: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "//val SPARK_HOME=sys.env.get(\"SPARK_HOME\").mkString\n",
    "val spark = SparkSession.builder()\n",
    "                        .appName(\"DE Work\")\n",
    "                        .enableHiveSupport()\n",
    "                        .getOrCreate()\n",
    "\n",
    "// 소스 데이터 경로\n",
    "val srcFilePath = \"..//src_data//2019-Oct.csv\"\n",
    "val df = spark.read.format(\"csv\")\n",
    "                    .option(\"header\",\"true\")\n",
    "                    .option(\"inferschema\",\"true\")  // 스키마 자동 추정\n",
    "                    .load(srcFilePath)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**스키마 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UTC -> KST 변환 컬럼 생성 (파티션 용 컬럼(yyyy-MM-dd))**\n",
    "  \n",
    "<고려사항>\n",
    "- utc time 에서 kst time 으로 변환 시 일부 데이터는 일, 월, 년도가 변경될 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|         event_time|kst_event_date|\n",
      "+-------------------+--------------+\n",
      "|2019-10-31 15:00:06|    2019-11-01|\n",
      "|2019-10-31 15:00:08|    2019-11-01|\n",
      "|2019-10-31 15:00:22|    2019-11-01|\n",
      "|2019-10-31 15:00:40|    2019-11-01|\n",
      "|2019-10-31 15:00:44|    2019-11-01|\n",
      "|2019-10-31 15:00:50|    2019-11-01|\n",
      "|2019-10-31 15:01:08|    2019-11-01|\n",
      "|2019-10-31 15:01:20|    2019-11-01|\n",
      "|2019-10-31 15:01:22|    2019-11-01|\n",
      "|2019-10-31 15:01:26|    2019-11-01|\n",
      "+-------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kst_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [event_time: timestamp, event_type: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\") // sql 쿼리로 데이터를 처리하기 위해 템프 뷰 생성\n",
    "val kst_df = spark.sql(\"\"\"select *, \n",
    "                                 to_date(from_utc_timestamp(event_time, 'Asia/Seoul')) as kst_event_date\n",
    "                             from df\n",
    "                       \"\"\").sample(0.01) // 테스트를 위한 데이터 샘플링 (약 10%)\n",
    "kst_df.createOrReplaceTempView(\"kst_df\") // sql 쿼리로 데이터를 처리하기 위해 템프 뷰 생성\n",
    "//println(kst_df.count()) //423,671\n",
    "\n",
    "// 파티션용 생성 컬럼 확인 및 날짜가 바뀌는 데이터 확인 (11월 1일로 바뀌는 데이터)\n",
    "spark.sql(\"\"\"select event_time,\n",
    "                    kst_event_date\n",
    "                from kst_df\n",
    "                where kst_event_date = \"2019-11-01\"\n",
    "          \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**스키마 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- kst_event_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kst_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parquet 형식의 snappy 압축 방식으로 데이터 처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/28 16:43:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "24/05/28 16:43:19 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "snkDataDir: String = ..//snk_data\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Parquet 데이터를 저장할 디렉토리 경로\n",
    "val snkDataDir = \"..//snk_data\"\n",
    "\n",
    "// 테이블 refresh\n",
    "//kst_df.cache()\n",
    "//spark.sql(\"refresh table kst_df\")\n",
    "\n",
    "// 결과 데이터(샘플) parquet 저장\n",
    "kst_df.write.option(\"compression\", \"snappy\") // 압축방식 snappy\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"kst_event_date\") // partition 컬럼\n",
    "            .parquet(s\"${snkDataDir}//\") // parquet 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HIVE External Table 생성**  \n",
    "  \n",
    "\\* 임의로 작성하였습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// DDL\n",
    "CREATE EXTERNAL TABLE user_atv_log(\n",
    "  event_time timestamp,\n",
    "  event_type string,\n",
    "  product_id string,\n",
    "  category_id string,\n",
    "  category_code string, \n",
    "  brand string,\n",
    "  price double, \n",
    "  user_id string,\n",
    "  user_session string\n",
    ")\n",
    "partitioned by(kst_event_date string)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/parquetDataPath/';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**추가 기간 처리 대응**  \n",
    "- 월별 데이터이므로 매월 특정 날짜,시간에 스케쥴러를 등록하여 배치처리\n",
    "- 소스데이터가 언제, 어디에, 어떻게 저장되는지에 따라 다른방법으로 개발\n",
    "- Airflow와 같은 데이터파이프라인 자동화 툴 사용 가능\n",
    "- 단순 스케쥴링을 필요로하는 작업은 Airflow를 사용하기 보다는 cron과 같은 job 스케쥴러를 사용할 수 있음\n",
    "- yyyy-MMM 파일명 형식의 csv 파일일 경우 아래 소스처럼 어플리케이션이 실행되는 날짜에 해당하는 대상 데이터 작업 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-Oct.csv\n",
      "2019-Nov.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import java.text.SimpleDateFormat\n",
       "import java.util.Locale\n",
       "oct_: String = 2019-10-01\n",
       "nov_: String = 2019-11-01\n",
       "inputFormat: java.text.SimpleDateFormat = java.text.SimpleDateFormat@f67a0200\n",
       "outputFormat: java.text.SimpleDateFormat = java.text.SimpleDateFormat@ef7955a0\n",
       "oct_file_name: String = 2019-Oct\n",
       "nov_file_name: String = 2019-Nov\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.text.SimpleDateFormat\n",
    "import java.util.Locale\n",
    "\n",
    "//val now_date = java.time.LocalDate.now.toString\n",
    "val oct_ = \"2019-10-01\"\n",
    "val nov_ = \"2019-11-01\"\n",
    "\n",
    "val inputFormat = new SimpleDateFormat(\"yyyy-MM-dd\")\n",
    "val outputFormat = new SimpleDateFormat(\"yyyy-MMM\", Locale.ENGLISH)\n",
    "\n",
    "//val work_file_name = outputFormat.format(inputFormat.parse(now_date))\n",
    "val oct_file_name = outputFormat.format(inputFormat.parse(oct_))\n",
    "val nov_file_name = outputFormat.format(inputFormat.parse(nov_))\n",
    "\n",
    "//println(s\"${work_file_name}.csv\") // 2024-May.csv\n",
    "println(s\"${oct_file_name}.csv\") // 2019-Oct.csv\n",
    "println(s\"${nov_file_name}.csv\") // 2019-Nov.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**배치 장애시 복구 방법**  \n",
    "- 대상 작업의 중요도에 따라 배치 장애 복구작업의 우선순위가 달라질 수 있음\n",
    "    - 원인파악이 먼저인지, 복구가 먼저인지\n",
    "- 해당 작업의 경우 프로그램 소스 앞단에 기본적인 예외처리 가능\n",
    "    - 대상 작업 소스 데이터 파일이 없는 경우 프로그램 종료\n",
    "- 배치 파이프라인 툴 Airflow 관련 기능 사용\n",
    "    - 성공, 실패 알림기능 설정 (이메일, 슬랙 등)\n",
    "    - task를 분리하여 빠른 원인파악 및 대응 가능\n",
    "    - dag_arguments에서 retry 관련 설정(retries, retry_delay 등) 및 excution_timeout(최대실행시간, 초과 시 fail) 관련 설정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
